package io.github.stream29.langchain4kt.api.openai

/**
 * Configuration for generating chat completions.
 *
 * @property model ID of the model to use.
 * @property temperature What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random,
 * while lower values like 0.2 will make it more focused and deterministic.
 * We generally recommend altering this or [topP] but not both.
 * @property topP An alternative to sampling with temperature, called nucleus sampling, where the model considers the results
 * of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.
 * We generally recommend altering this or [temperature] but not both.
 * @property stop Up to 4 sequences where the API will stop generating further tokens.
 * @property maxTokens The maximum number of tokens allowed for the generated answer. By default, the number of tokens the model can
 * return will be (4096 - prompt tokens).
 * @property presencePenalty Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far,
 * increasing the model's likelihood to talk about new topics.
 * [Read more](https://platform.openai.com/docs/api-reference/parameter-details)
 * @property frequencyPenalty Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far,
 * decreasing the model's likelihood to repeat the same line verbatim.
 * [Read more](https://platform.openai.com/docs/api-reference/parameter-details)
 * @property logitBias Modify the likelihood of specified tokens appearing in the completion.
 * Accepts a json object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value
 * from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling.
 * The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection;
 * values like -100 or 100 should result in a ban or exclusive selection of the relevant token.
 * @property user A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse.
 * @property seed If specified, our system will make the best effort to sample deterministically, such that repeated requests with
 * the same seed and parameters should return the same result. Determinism is not guaranteed, and you should refer to the `systemFingerprint`
 * response parameter to monitor changes in the backend.
 * @property logprobs Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token
 * returned in the content of message.
 * @property topLogprobs An integer between 0 and 20 specifying the number of most likely tokens to return at each token position,
 * each with an associated log probability. logprobs must be set to true if this parameter is used.
 * @property instanceId A unique identifier representing the Multi LORA reserved instance.
 */
public data class OpenAiGenerationConfig(
    public val model: String,
    public val temperature: Double? = null,
    public val topP: Double? = null,
    public val stop: List<String>? = null,
    public val maxTokens: Int? = null,
    public val presencePenalty: Double? = null,
    public val frequencyPenalty: Double? = null,
    public val logitBias: Map<String, Int>? = null,
    public val user: String? = null,
    public val seed: Int? = null,
    public val logprobs: Boolean? = null,
    public val topLogprobs: Int? = null,
    public val instanceId: String? = null,
)
